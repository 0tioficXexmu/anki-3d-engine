// Copyright (C) 2009-2021, Panagiotis Christopoulos Charitos and contributors.
// All rights reserved.
// Code licensed under the BSD License.
// http://www.anki3d.org/LICENSE

// Does SSGI and GI probe sampling

#define REMOVE_FIREFLIES true
#define RECALCULATE_NORMAL false

#pragma anki start comp
#include <AnKi/Shaders/SsRaymarching.glsl>
#include <AnKi/Shaders/Functions.glsl>
#include <AnKi/Shaders/PackFunctions.glsl>
#include <AnKi/Shaders/ImportanceSampling.glsl>
#include <AnKi/Shaders/TonemappingFunctions.glsl>
#include <AnKi/Shaders/Include/IndirectDiffuseTypes.h>

const UVec2 WORKGROUP_SIZE = UVec2(8, 8);
layout(local_size_x = WORKGROUP_SIZE.x, local_size_y = WORKGROUP_SIZE.y) in;

#define CLUSTERED_SHADING_SET 0
#define CLUSTERED_SHADING_UNIFORMS_BINDING 0
#define CLUSTERED_SHADING_GI_BINDING 1
#define CLUSTERED_SHADING_CLUSTERS_BINDING 3
#include <AnKi/Shaders/ClusteredShadingCommon.glsl>

layout(set = 0, binding = 4) writeonly uniform image2D u_outImage;
layout(set = 0, binding = 5) writeonly uniform image2D u_momentsAndHistoryLengthImage;

layout(set = 0, binding = 6) uniform sampler u_trilinearClampSampler;
layout(set = 0, binding = 7) uniform texture2D u_gbufferRt2;
layout(set = 0, binding = 8) uniform texture2D u_depthRt;
layout(set = 0, binding = 9) uniform texture2D u_lightBufferRt;
layout(set = 0, binding = 10) uniform texture2D u_historyTex;
layout(set = 0, binding = 11) uniform texture2D u_motionVectorsTex;
layout(set = 0, binding = 12) uniform texture2D u_motionVectorRejectionTex;
layout(set = 0, binding = 13) uniform texture2D u_prevMomentsAndHistoryLengthTex;

layout(push_constant, std430) uniform b_pc
{
	IndirectDiffuseUniforms u_unis;
};

void main()
{
	const UVec2 fixedGlobalInvocationId = min(gl_GlobalInvocationID.xy, u_unis.m_viewportSize);
	const Vec2 fragCoord = Vec2(fixedGlobalInvocationId.xy) + 0.5;
	const Vec2 uv = fragCoord / u_unis.m_viewportSizef;
	const Vec2 ndc = UV_TO_NDC(uv);

	// Get normal
	const Vec3 worldNormal = readNormalFromGBuffer(u_gbufferRt2, u_trilinearClampSampler, uv);
	const Vec3 viewNormal = u_clusteredShading.m_matrices.m_viewRotation * worldNormal;

	// Get depth
	const F32 depth = textureLod(u_depthRt, u_trilinearClampSampler, uv, 0.0).r;

	// Compute view pos
	const Vec4 viewPos4 = u_clusteredShading.m_matrices.m_invertedProjectionJitter * Vec4(ndc, depth, 1.0);
	const Vec3 viewPos = viewPos4.xyz / viewPos4.w;

	// Get a random point inside the hemisphere. Use hemisphereSampleCos to avoid perpendicular vecs to viewNormal
	const UVec2 random = rand3DPCG16(UVec3(fixedGlobalInvocationId, u_clusteredShading.m_frame)).xy;
	Vec2 randomCircle = hammersleyRandom16(0u, 0xFFFFu, random);
	randomCircle.x *= 0.9; // Reduce the cone angle a bit to avoid self-collisions
	randomCircle.x = pow(randomCircle.x, 4.0); // Get more samples closer to the normal
	const Vec3 randomHemisphere = rotationFromDirection(viewNormal) * hemisphereSampleCos(randomCircle);

	// Trace
	Vec3 hitPoint;
	F32 hitAttenuation;
	const U32 lod = 0u;
	const F32 minStepf = 4.0;
	const F32 noise = F32(random.x) * (1.0 / 65536.0);
	const U32 initialStep = U32(mix(minStepf, F32(u_unis.m_stepIncrement), noise));
	raymarchGroundTruth(viewPos, randomHemisphere, uv, depth, u_clusteredShading.m_matrices.m_projectionJitter,
						u_unis.m_maxSteps, u_depthRt, u_trilinearClampSampler, F32(lod), u_unis.m_depthBufferSize,
						u_unis.m_stepIncrement, initialStep, hitPoint, hitAttenuation);

	// Reject backfacing
	ANKI_BRANCH if(hitAttenuation > 0.0)
	{
		const Vec3 hitNormal = u_clusteredShading.m_matrices.m_viewRotation
							   * readNormalFromGBuffer(u_gbufferRt2, u_trilinearClampSampler, hitPoint.xy);
		F32 backFaceAttenuation;
		rejectBackFaces(randomHemisphere, hitNormal, backFaceAttenuation);

		hitAttenuation *= backFaceAttenuation;
	}

	// Read the light buffer
	Vec3 outColor;
	ANKI_BRANCH if(hitAttenuation > 0.0)
	{
		// Reproject the UV because you are reading the previous frame
		const Vec4 v4 = u_clusteredShading.m_matrices.m_reprojection * Vec4(UV_TO_NDC(hitPoint.xy), hitPoint.z, 1.0);
		hitPoint.xy = NDC_TO_UV(v4.xy / v4.w);

		// Read the light buffer
		outColor = textureLod(u_lightBufferRt, u_trilinearClampSampler, hitPoint.xy, 100.0).rgb;
		outColor = clamp(outColor, 0.0, FLT_MAX); // Fix the value just in case
		outColor *= hitAttenuation;

		// Compute a new normal based on the new hit point
		Vec3 newViewNormal;
		if(RECALCULATE_NORMAL)
		{
			const F32 depth = textureLod(u_depthRt, u_trilinearClampSampler, hitPoint.xy, 0.0).r;
			const Vec4 viewPos4 =
				u_clusteredShading.m_matrices.m_invertedProjectionJitter * Vec4(UV_TO_NDC(hitPoint.xy), depth, 1.0);
			const Vec3 hitViewPos = viewPos4.xyz / viewPos4.w;
			newViewNormal = normalize(hitViewPos - viewPos);
		}
		else
		{
			newViewNormal = viewNormal;
		}

		// Modulate
		const F32 NoL = max(0.0, dot(randomHemisphere, newViewNormal));
		outColor *= NoL;
		outColor *= 2.0 * PI;
	}
	else
	{
		// Fallback to probes

		// Get the cluster
		Cluster cluster = getClusterFragCoord(Vec3(fragCoord, depth));

		// Get world position
		const Vec4 worldPos4 = u_clusteredShading.m_matrices.m_invertedViewProjectionJitter * Vec4(ndc, depth, 1.0);
		const Vec3 worldPos = worldPos4.xyz / worldPos4.w;

		if(bitCount(cluster.m_giProbesMask) == 1)
		{
			// All subgroups point to the same probe and there is only one probe, do a fast path without blend weight

			const GlobalIlluminationProbe probe = u_giProbes[findLSB2(cluster.m_giProbesMask)];

			// Sample
			outColor = sampleGlobalIllumination(worldPos, worldNormal, probe, u_globalIlluminationTextures,
												u_trilinearClampSampler);
		}
		else
		{
			// Zero or more than one probes, do a slow path that blends them together

			F32 totalBlendWeight = EPSILON;
			outColor = Vec3(0.0);

			// Loop probes
			ANKI_LOOP while(cluster.m_giProbesMask != 0u)
			{
				const U32 idx = U32(findLSB2(cluster.m_giProbesMask));
				cluster.m_giProbesMask &= ~(1u << idx);
				const GlobalIlluminationProbe probe = u_giProbes[idx];

				// Compute blend weight
				const F32 blendWeight =
					computeProbeBlendWeight(worldPos, probe.m_aabbMin, probe.m_aabbMax, probe.m_fadeDistance);
				totalBlendWeight += blendWeight;

				// Sample
				const Vec3 c = sampleGlobalIllumination(worldPos, worldNormal, probe, u_globalIlluminationTextures,
														u_trilinearClampSampler);
				outColor += c * blendWeight;
			}

			// Normalize
			outColor /= totalBlendWeight;
		}
	}

	// Remove fireflies
	if(REMOVE_FIREFLIES)
	{
		const F32 lum = computeLuminance(outColor) + 0.001;
		const F32 averageLum = (subgroupAdd(lum) / F32(gl_SubgroupSize)) * 2.0;
		const F32 newLum = min(lum, averageLum);
		outColor *= newLum / lum;
	}

	// Compute history length
	const Vec2 historyUv = uv + textureLod(u_motionVectorsRt, u_linearAnyClampSampler, uv, 0.0).xy;
	const F32 historyRejectionFactor = textureLod(u_motionVectorsRejectionRt, u_linearAnyClampSampler, uv, 0.0).x;
	const Vec3 prevMomentsAndHistoryLength =
		textureLod(u_momentsAndHistoryLengthTex, u_linearAnyClampSampler, historyUv, 0.0).xyz;
	F32 historyLength;
	if(historyRejectionFactor >= 0.5)
	{
		// Rejection factor too high, reset the temporal history
		historyLength = 1.0;
	}
	else
	{
		// Sample seems stable, increment its temporal history
		historyLength = prevMomentsAndHistoryLength.z + 1.0;
	}

	// Blend color with history
	{
		// Compute blend fractor. Use nearest sampler because it's an integer texture
		const F32 lowestBlendFactor = 0.1;
		const F32 stableFrames = 4.0;
		const F32 lerp = min(1.0, historyLength / stableFrames);
		const F32 blendFactor = mix(1.0, lowestBlendFactor, lerp);

		// Blend with history
		const Vec3 history = textureLod(u_historyTex, u_trilinearClampSampler, historyUv, 0.0).rgb;
		outColor = mix(history, outColor, blendFactor);
	}

	// Store color
	imageStore(u_outImage, IVec2(fixedGlobalInvocationId), Vec4(outColor, 1.0));

	// Compute the moments that will give temporal variance
	Vec2 moments;
	moments.x = computeLuminance(outColor);
	moments.y = moments.x * moments.x;

	// Blend the moments with history
	const F32 momentsBlendFactor = 0.2;
	momentsBlendFactor = mix(momentsBlendFactor, 1.0, historyRejectionFactor);
	moments = mix(prevMomentsAndHistoryLength.xy, moments, momentsBlendFactor);

	// Store the moments + history len
	imageStore(u_momentsAndHistoryLengthImage, IVec2(fixedGlobalInvocationId), Vec4(moments, historyLength, 0.0));
}

#pragma anki end
